{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Challenge 0 - Load,Query and Create connection of your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A lot of the times you won't have files already saved in an Excel or CSV file for you to prepare your data, implying that a lot of times you'll be extracting data from a Data Warehouse or Data Lake, majority of times through SQL.\n",
    "#### A couple of times you may want to do some queries on your table in mySQL to get a slight view on what you have in hands,\n",
    "#### so let's simulate that!\n",
    "\n",
    "#### First we'll need to create a database and table in mySQL:\n",
    "\n",
    "##### 1º- Open the austin_weather.sql file in MySQL Workbench and run the script into a desired schema.\n",
    "\n",
    "#### 2º- As we are in mySQL Workbench, we can do some queries there, to get an overview on some characteristics of our data:\n",
    " - a) How many days are recorded in the dataset?\n",
    " - b) What is the day with the Highest Temperature in Fahrenheit (column TempHighF)\n",
    " - c) What is the average Humidity across all days? (column HumidityAvgPercent)\n",
    " - d) Top 10 days, where SeaLevelPressureAvgInches is the highest, knowing DewPointAvgF is higher than 28 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answers below:\n",
    "\n",
    "# a) 1319\n",
    "# b)\n",
    "# c)66.5708\n",
    "# d)2016-11-19 00:00:00\t30.54\n",
    "2016-12-08 00:00:00\t30.49\n",
    "2014-12-31 00:00:00\t30.48\n",
    "2017-03-03 00:00:00\t30.48\n",
    "2016-01-01 00:00:00\t30.48\n",
    "2015-01-04 00:00:00\t30.48\n",
    "2014-01-23 00:00:00\t30.47\n",
    "2014-11-27 00:00:00\t30.45\n",
    "2015-12-04 00:00:00\t30.45\n",
    "2015-02-23 00:00:00\t30.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you explored a couple of elements in your table, let's bring your table into this jupyter notebook, by creating a Python-SQL connection like you did on MySQL Project!\n",
    "#### In case you need a little refresher check this [link](https://www.dataquest.io/blog/sql-insert-tutorial/).\n",
    "##### 1º - Create a connection using sqlalchemy from python to mysql \n",
    "##### 2º- Load the table into a variable called weather_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "DATABASE_URL = \"mysql+pymysql://root:redhotblink@localhost/ironhack\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "saved_password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_sql(\"SELECT * FROM austin_weather\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Challenge 1 - Describe the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the dataset you have loaded: \n",
    "- Look at the variables and their types\n",
    "- Examine the descriptive statistics of the numeric variables \n",
    "- Look at the first five rows of all variables to evaluate the categorical variables as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1319 entries, 0 to 1318\n",
      "Data columns (total 21 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   Date                        1319 non-null   datetime64[ns]\n",
      " 1   TempHighF                   1319 non-null   int64         \n",
      " 2   TempAvgF                    1319 non-null   int64         \n",
      " 3   TempLowF                    1319 non-null   int64         \n",
      " 4   DewPointHighF               1319 non-null   object        \n",
      " 5   DewPointAvgF                1319 non-null   object        \n",
      " 6   DewPointLowF                1319 non-null   object        \n",
      " 7   HumidityHighPercent         1319 non-null   object        \n",
      " 8   HumidityAvgPercent          1319 non-null   object        \n",
      " 9   HumidityLowPercent          1319 non-null   object        \n",
      " 10  SeaLevelPressureHighInches  1319 non-null   object        \n",
      " 11  SeaLevelPressureAvgInches   1319 non-null   object        \n",
      " 12  SeaLevelPressureLowInches   1319 non-null   object        \n",
      " 13  VisibilityHighMiles         1319 non-null   object        \n",
      " 14  VisibilityAvgMiles          1319 non-null   object        \n",
      " 15  VisibilityLowMiles          1319 non-null   object        \n",
      " 16  WindHighMPH                 1319 non-null   object        \n",
      " 17  WindAvgMPH                  1319 non-null   object        \n",
      " 18  WindGustMPH                 1319 non-null   object        \n",
      " 19  PrecipitationSumInches      1319 non-null   object        \n",
      " 20  Events                      1319 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(3), object(17)\n",
      "memory usage: 216.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TempHighF</th>\n",
       "      <th>TempAvgF</th>\n",
       "      <th>TempLowF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1319.000000</td>\n",
       "      <td>1319.000000</td>\n",
       "      <td>1319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80.862775</td>\n",
       "      <td>70.642911</td>\n",
       "      <td>59.902957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.766523</td>\n",
       "      <td>14.045904</td>\n",
       "      <td>14.190648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>92.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>107.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TempHighF     TempAvgF     TempLowF\n",
       "count  1319.000000  1319.000000  1319.000000\n",
       "mean     80.862775    70.642911    59.902957\n",
       "std      14.766523    14.045904    14.190648\n",
       "min      32.000000    29.000000    19.000000\n",
       "25%      72.000000    62.000000    49.000000\n",
       "50%      83.000000    73.000000    63.000000\n",
       "75%      92.000000    83.000000    73.000000\n",
       "max     107.000000    93.000000    81.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>TempHighF</th>\n",
       "      <th>TempAvgF</th>\n",
       "      <th>TempLowF</th>\n",
       "      <th>DewPointHighF</th>\n",
       "      <th>DewPointAvgF</th>\n",
       "      <th>DewPointLowF</th>\n",
       "      <th>HumidityHighPercent</th>\n",
       "      <th>HumidityAvgPercent</th>\n",
       "      <th>HumidityLowPercent</th>\n",
       "      <th>...</th>\n",
       "      <th>SeaLevelPressureAvgInches</th>\n",
       "      <th>SeaLevelPressureLowInches</th>\n",
       "      <th>VisibilityHighMiles</th>\n",
       "      <th>VisibilityAvgMiles</th>\n",
       "      <th>VisibilityLowMiles</th>\n",
       "      <th>WindHighMPH</th>\n",
       "      <th>WindAvgMPH</th>\n",
       "      <th>WindGustMPH</th>\n",
       "      <th>PrecipitationSumInches</th>\n",
       "      <th>Events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-21</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>49</td>\n",
       "      <td>43</td>\n",
       "      <td>93</td>\n",
       "      <td>75</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>29.68</td>\n",
       "      <td>29.59</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>0.46</td>\n",
       "      <td>Rain , Thunderstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-22</td>\n",
       "      <td>56</td>\n",
       "      <td>48</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>93</td>\n",
       "      <td>68</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>30.13</td>\n",
       "      <td>29.87</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-23</td>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>76</td>\n",
       "      <td>52</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>30.49</td>\n",
       "      <td>30.41</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-24</td>\n",
       "      <td>61</td>\n",
       "      <td>46</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>89</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>30.45</td>\n",
       "      <td>30.3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-25</td>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>86</td>\n",
       "      <td>71</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>30.33</td>\n",
       "      <td>30.27</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  TempHighF  TempAvgF  TempLowF DewPointHighF DewPointAvgF  \\\n",
       "0 2013-12-21         74        60        45            67           49   \n",
       "1 2013-12-22         56        48        39            43           36   \n",
       "2 2013-12-23         58        45        32            31           27   \n",
       "3 2013-12-24         61        46        31            36           28   \n",
       "4 2013-12-25         58        50        41            44           40   \n",
       "\n",
       "  DewPointLowF HumidityHighPercent HumidityAvgPercent HumidityLowPercent  ...  \\\n",
       "0           43                  93                 75                 57  ...   \n",
       "1           28                  93                 68                 43  ...   \n",
       "2           23                  76                 52                 27  ...   \n",
       "3           21                  89                 56                 22  ...   \n",
       "4           36                  86                 71                 56  ...   \n",
       "\n",
       "  SeaLevelPressureAvgInches SeaLevelPressureLowInches VisibilityHighMiles  \\\n",
       "0                     29.68                     29.59                  10   \n",
       "1                     30.13                     29.87                  10   \n",
       "2                     30.49                     30.41                  10   \n",
       "3                     30.45                      30.3                  10   \n",
       "4                     30.33                     30.27                  10   \n",
       "\n",
       "  VisibilityAvgMiles VisibilityLowMiles WindHighMPH WindAvgMPH WindGustMPH  \\\n",
       "0                  7                  2          20          4          31   \n",
       "1                 10                  5          16          6          25   \n",
       "2                 10                 10           8          3          12   \n",
       "3                 10                  7          12          4          20   \n",
       "4                 10                  7          10          2          16   \n",
       "\n",
       "  PrecipitationSumInches               Events  \n",
       "0                   0.46  Rain , Thunderstorm  \n",
       "1                      0                       \n",
       "2                      0                       \n",
       "3                      0                       \n",
       "4                      T                       \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "weather_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given the information you have learned from examining the dataset, write down three insights about the data in a markdown cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Insights:\n",
    "\n",
    "1. There are 21 variables in the dataset. 3 of them are numeric and the rest contain some text.\n",
    "\n",
    "2. The average temperature in Austin ranged between around 70 degrees F and around 93 degrees F. The highest temperature observed during this period was 107 degrees F and the lowest was 19 degrees F.\n",
    "\n",
    "3. When we look at the head function, we see that a lot of variables contain numeric data even though these columns are of object type. This means we might have to do some data cleansing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's examine the DewPointAvgF variable by using the `unique()` function to list all unique values in this dataframe.\n",
    "\n",
    "Describe what you find in a markdown cell below the code. What did you notice? What do you think made Pandas to treat this column as *object* instead of *int64*? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['49', '36', '27', '28', '40', '39', '41', '26', '42', '22', '48',\n",
       "       '32', '8', '11', '45', '55', '61', '37', '47', '25', '23', '20',\n",
       "       '33', '30', '29', '17', '14', '13', '54', '59', '15', '24', '34',\n",
       "       '35', '57', '50', '53', '60', '46', '56', '51', '31', '38', '62',\n",
       "       '43', '63', '64', '67', '66', '58', '70', '68', '65', '69', '71',\n",
       "       '72', '-', '73', '74', '21', '44', '52', '12', '75', '76', '18'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "weather_df['DewPointAvgF'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observation here\n",
    "#It looks like the empty values are displayed as '-' maybe replacing them with a 0 or geting rid of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a list of columns misrepresented as `object`. Use this list to convert the columns to numeric using the `pandas.to_numeric` function in the next cell. If you encounter errors in converting strings to numeric values, you need to catch those errors and force the conversion by supplying `errors='coerce'` as an argument for `pandas.to_numeric`. Coercing will replace non-convertable elements with `NaN` which represents an undefined numeric value. This makes it possible for us to conveniently handle missing values in subsequent data processing.\n",
    "\n",
    "*Hint: you may use a loop to change one column at a time but it is more efficient to use `apply`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_type_columns = ['DewPointHighF', 'DewPointAvgF', 'DewPointLowF', 'HumidityHighPercent', \n",
    "                      'HumidityAvgPercent', 'HumidityLowPercent', 'SeaLevelPressureHighInches', \n",
    "                      'SeaLevelPressureAvgInches' ,'SeaLevelPressureLowInches', 'VisibilityHighMiles',\n",
    "                      'VisibilityAvgMiles', 'VisibilityLowMiles', 'WindHighMPH', 'WindAvgMPH', \n",
    "                      'WindGustMPH', 'PrecipitationSumInches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "weather_df[wrong_type_columns] = weather_df[wrong_type_columns].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if your code has worked by printing the data types again. You should see only two `object` columns (`Date` and `Events`) now. All other columns should be `int64` or `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1319 entries, 0 to 1318\n",
      "Data columns (total 21 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   Date                        1319 non-null   datetime64[ns]\n",
      " 1   TempHighF                   1319 non-null   int64         \n",
      " 2   TempAvgF                    1319 non-null   int64         \n",
      " 3   TempLowF                    1319 non-null   int64         \n",
      " 4   DewPointHighF               1312 non-null   float64       \n",
      " 5   DewPointAvgF                1312 non-null   float64       \n",
      " 6   DewPointLowF                1312 non-null   float64       \n",
      " 7   HumidityHighPercent         1317 non-null   float64       \n",
      " 8   HumidityAvgPercent          1317 non-null   float64       \n",
      " 9   HumidityLowPercent          1317 non-null   float64       \n",
      " 10  SeaLevelPressureHighInches  1316 non-null   float64       \n",
      " 11  SeaLevelPressureAvgInches   1316 non-null   float64       \n",
      " 12  SeaLevelPressureLowInches   1316 non-null   float64       \n",
      " 13  VisibilityHighMiles         1307 non-null   float64       \n",
      " 14  VisibilityAvgMiles          1307 non-null   float64       \n",
      " 15  VisibilityLowMiles          1307 non-null   float64       \n",
      " 16  WindHighMPH                 1317 non-null   float64       \n",
      " 17  WindAvgMPH                  1317 non-null   float64       \n",
      " 18  WindGustMPH                 1315 non-null   float64       \n",
      " 19  PrecipitationSumInches      1195 non-null   float64       \n",
      " 20  Events                      1319 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(16), int64(3), object(1)\n",
      "memory usage: 216.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Handle the Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have fixed the type mismatch, let's address the missing data.\n",
    "\n",
    "By coercing the columns to numeric, we have created `NaN` for each cell containing characters. We should choose a strategy to address these missing data.\n",
    "\n",
    "The first step is to examine how many rows contain missing data.\n",
    "\n",
    "We check how much missing data we have by applying the `.isnull()` function to our dataset. To find the rows with missing data in any of its cells, we apply `.any(axis=1)` to the function. `austin.isnull().any(axis=1)` will return a column containing true if the row contains at least one missing value and false otherwise. Therefore we must subset our dataframe with this column. This will give us all rows with at least one missing value. \n",
    "\n",
    "#### In the next cell, identify all rows containing at least one missing value. Assign the dataframes with missing values to a variable called `missing_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 21)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "missing_values = weather_df[weather_df.isnull().any(axis=1)]\n",
    "\n",
    "missing_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple strategies to handle missing data. Below lists the most common ones data scientists use:\n",
    "\n",
    "* Removing all rows or all columns containing missing data. This is the simplest strategy. It may work in some cases but not others.\n",
    "\n",
    "* Filling all missing values with a placeholder value. \n",
    "    * For categorical data, `0`, `-1`, and `9999` are some commonly used placeholder values. \n",
    "    * For continuous data, some may opt to fill all missing data with the mean. This strategy is not optimal since it can increase the fit of the model.\n",
    "\n",
    "* Filling the values using some algorithm. \n",
    "\n",
    "#### In our case, we will use a hybrid approach which is to first remove the data that contain most missing values then fill in the rest of the missing values with the *linear interpolation* algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, count the number of rows of `austin` and `missing_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1319, 136)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "total_rows = weather_df.shape[0]\n",
    "missing_rows = missing_values.shape[0]\n",
    "\n",
    "total_rows, missing_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the ratio of missing rows to total rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10310841546626232"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "missing_ratio = missing_rows / total_rows\n",
    "\n",
    "missing_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a large proportion of missing data (over 10%). Perhaps we should evaluate which columns have the most missing data and remove those columns. For the remaining columns, we will perform a linear approximation of the missing data.\n",
    "\n",
    "We can find the number of missing rows in each column using the `.isna()` function. We then chain the `.sum` function to the `.isna()` function and find the number of missing rows per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                            0\n",
       "TempHighF                       0\n",
       "TempAvgF                        0\n",
       "TempLowF                        0\n",
       "DewPointHighF                   7\n",
       "DewPointAvgF                    7\n",
       "DewPointLowF                    7\n",
       "HumidityHighPercent             2\n",
       "HumidityAvgPercent              2\n",
       "HumidityLowPercent              2\n",
       "SeaLevelPressureHighInches      3\n",
       "SeaLevelPressureAvgInches       3\n",
       "SeaLevelPressureLowInches       3\n",
       "VisibilityHighMiles            12\n",
       "VisibilityAvgMiles             12\n",
       "VisibilityLowMiles             12\n",
       "WindHighMPH                     2\n",
       "WindAvgMPH                      2\n",
       "WindGustMPH                     4\n",
       "PrecipitationSumInches        124\n",
       "Events                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "missing_per_column = weather_df.isna().sum()\n",
    "\n",
    "missing_per_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see from the output, the majority of missing data is in one column called `PrecipitationSumInches`. What's the number of missing values in this column in ratio to its total number of rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09401061410159212"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "precipitation_missing_ratio = missing_per_column['PrecipitationSumInches'] / total_rows\n",
    "\n",
    "precipitation_missing_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 10% data missing! Therefore, we prefer to remove this column instead of filling its missing values. \n",
    "\n",
    "#### Remove this column from `austin` using the `.drop()` function. Use the `inplace=True` argument.\n",
    "\n",
    "*Hints:*\n",
    "\n",
    "* By supplying `inplace=True` to `drop()`, the original dataframe object will be changed in place and the function will return `None`. In contrast, if you don't supply `inplace=True`, which is equivalent to supplying `inplace=False` because `False` is the default value, the original dataframe object will be kept and the function returns a copy of the transformed dataframe object. In the latter case, you'll have to assign the returned object back to your variable.\n",
    "\n",
    "* Also, since you are dropping a column instead of a row, you'll need to supply `axis=1` to `drop()`.\n",
    "\n",
    "[Reference for `pandas.DataFrame.drop`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'austin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m weather_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecipitationSumInches\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print `austin` to confirm the column is indeed removed\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43maustin\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'austin' is not defined"
     ]
    }
   ],
   "source": [
    "# Your code here \n",
    "weather_df.drop('PrecipitationSumInches', axis=1, inplace=True)\n",
    "\n",
    "# Print `austin` to confirm the column is indeed removed\n",
    "\n",
    "print(austin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we will perform linear interpolation of the missing data.\n",
    "\n",
    "This means that we will use a linear algorithm to estimate the missing data. Linear interpolation assumes that there is a straight line between the points and the missing point will fall on that line. This is a good enough approximation for weather related data. Weather related data is typically a time series. Therefore, we do not want to drop rows from our data if possible. It is prefereable to estimate the missing values rather than remove the rows. However, if you have data from a single point in time, perhaps a better solution would be to remove the rows. \n",
    "\n",
    "If you would like to read more about linear interpolation, you can do so [here](https://en.wikipedia.org/wiki/Linear_interpolation).\n",
    "\n",
    "In the following cell, use the `.interpolate()` function on the entire dataframe. This time pass the `inplace=False` argument to the function and assign the interpolated dataframe to a new variable called `austin_fixed` so that we can compare with `austin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid fill method. Expecting pad (ffill) or backfill (bfill). Got linear",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data_as_array \u001b[38;5;241m=\u001b[39m weather_df\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Apply linear interpolation on numpy array\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m interpolated_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_as_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert back to dataframe\u001b[39;00m\n\u001b[0;32m     11\u001b[0m austin_fixed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(interpolated_data, columns\u001b[38;5;241m=\u001b[39mweather_df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind0], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[0;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "Cell \u001b[1;32mIn[54], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m      5\u001b[0m data_as_array \u001b[38;5;241m=\u001b[39m weather_df\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Apply linear interpolation on numpy array\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m interpolated_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m col: \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m0\u001b[39m, data_as_array)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert back to dataframe\u001b[39;00m\n\u001b[0;32m     11\u001b[0m austin_fixed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(interpolated_data, columns\u001b[38;5;241m=\u001b[39mweather_df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:6063\u001b[0m, in \u001b[0;36mSeries.interpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   6051\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   6052\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minterpolate\u001b[39m(\n\u001b[0;32m   6053\u001b[0m     \u001b[38;5;28mself\u001b[39m: Series,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6061\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   6062\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 6063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[0;32m   6064\u001b[0m         method,\n\u001b[0;32m   6065\u001b[0m         axis,\n\u001b[0;32m   6066\u001b[0m         limit,\n\u001b[0;32m   6067\u001b[0m         inplace,\n\u001b[0;32m   6068\u001b[0m         limit_direction,\n\u001b[0;32m   6069\u001b[0m         limit_area,\n\u001b[0;32m   6070\u001b[0m         downcast,\n\u001b[0;32m   6071\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   6072\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:7568\u001b[0m, in \u001b[0;36mNDFrame.interpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   7562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isna(index)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   7564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterpolation with NaNs in the index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   7565\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas not been implemented. Try filling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   7566\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthose NaNs before interpolating.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   7567\u001b[0m     )\n\u001b[1;32m-> 7568\u001b[0m new_data \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[0;32m   7569\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m   7570\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   7571\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   7572\u001b[0m     limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m   7573\u001b[0m     limit_direction\u001b[38;5;241m=\u001b[39mlimit_direction,\n\u001b[0;32m   7574\u001b[0m     limit_area\u001b[38;5;241m=\u001b[39mlimit_area,\n\u001b[0;32m   7575\u001b[0m     inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   7576\u001b[0m     downcast\u001b[38;5;241m=\u001b[39mdowncast,\n\u001b[0;32m   7577\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   7578\u001b[0m )\n\u001b[0;32m   7580\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\n\u001b[0;32m   7581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_transpose:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:422\u001b[0m, in \u001b[0;36mBaseBlockManager.interpolate\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minterpolate\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1619\u001b[0m, in \u001b[0;36mEABackedBlock.interpolate\u001b[1;34m(self, method, axis, inplace, limit, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mfillna(value\u001b[38;5;241m=\u001b[39mfill_value, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1619\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block_same_class(new_values)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:317\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.fillna\u001b[1;34m(self, value, method, limit)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;129m@doc\u001b[39m(ExtensionArray\u001b[38;5;241m.\u001b[39mfillna)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfillna\u001b[39m(\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDArrayBackedExtensionArrayT, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDArrayBackedExtensionArrayT:\n\u001b[1;32m--> 317\u001b[0m     value, method \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_fillna_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_scalar_dict_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misna()\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# error: Argument 2 to \"check_value_size\" has incompatible type\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# \"ExtensionArray\"; expected \"ndarray\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_validators.py:390\u001b[0m, in \u001b[0;36mvalidate_fillna_kwargs\u001b[1;34m(value, method, validate_scalar_dict_value)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify a fill \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[43mclean_fill_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_scalar_dict_value \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\missing.py:125\u001b[0m, in \u001b[0;36mclean_fill_method\u001b[1;34m(method, allow_nearest)\u001b[0m\n\u001b[0;32m    123\u001b[0m     expecting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad (ffill), backfill (bfill) or nearest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_methods:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid fill method. Expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpecting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid fill method. Expecting pad (ffill) or backfill (bfill). Got linear"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import numpy as np\n",
    "\n",
    "# Convert dataframe to numpy array\n",
    "data_as_array = weather_df.values\n",
    "\n",
    "# Apply linear interpolation on numpy array\n",
    "interpolated_data = np.apply_along_axis(lambda col: pd.Series(col).interpolate(method='linear').values, 0, data_as_array)\n",
    "\n",
    "# Convert back to dataframe\n",
    "austin_fixed = pd.DataFrame(interpolated_data, columns=weather_df.columns)\n",
    "\n",
    "austin_fixed.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to make sure `austin_fixed` contains no missing data. Also check `austin` - it still contains missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Processing the `Events` Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our dataframe contains one true text column - the Events column. We should evaluate this column to determine how to process it.\n",
    "\n",
    "Use the `value_counts()` function to evaluate the contents of this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the values of `Events` and reflecting what those values mean in the context of data, you realize this column indicates what weather events had happened in a particular day.\n",
    "\n",
    "#### What is the largest number of events happened in a single day? Enter your answer in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to transform the string-type `Events` values to the numbers. This will allow us to apply machine learning algorithms easily.\n",
    "\n",
    "How? We will create a new column for each type of events (i.e. *Rain*, *Snow*, *Fog*, *Thunderstorm*. In each column, we use `1` to indicate if the corresponding event happened in that day and use `0` otherwise.\n",
    "\n",
    "Below we provide you a list of all event types. Loop the list and create a dummy column with `0` values for each event in `austin_fixed`. To create a new dummy column with `0` values, simply use `austin_fixed[event] = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = ['Snow', 'Fog', 'Rain', 'Thunderstorm']\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Print your new dataframe to check whether new columns have been created:\n",
    "\n",
    "austin_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, populate the actual values in the dummy columns of  `austin_fixed`.\n",
    "\n",
    "You will check the *Events* column. If its string value contains `Rain`, then the *Rain* column should be `1`. The same for `Snow`, `Fog`, and `Thunderstorm`.\n",
    "\n",
    "*Hints:*\n",
    "\n",
    "* Use [`pandas.Series.str.contains()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html) to create the value series of each new column.\n",
    "\n",
    "* What if the values you populated are booleans instead of numbers? You can cast the boolean values to numbers by using `.astype(int)`. For instance, `pd.Series([True, True, False]).astype(int)` will return a new series with values of `[1, 1, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out `austin_fixed` to check if the event columns are populated with the intended values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your code worked correctly, now we can drop the `Events` column as we don't need it any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Processing The `Date` Column\n",
    "\n",
    "The `Date` column is another non-numeric field in our dataset. A value in that field looks like `'2014-01-06'` which consists of the year, month, and day connected with hyphens. One way to convert the date string to numerical is using a similar approach as we used for `Events`, namely splitting the column into numerical `Year`, `Month`, and `Day` columns. In this challenge we'll show you another way which is to use the Python `datetime` library's `toordinal()` function. Depending on what actual machine learning analysis you will conduct, each approach has its pros and cons. Our goal today is to practice data preparation so we'll skip the discussion here.\n",
    "\n",
    "Here you can find the [reference](https://docs.python.org/3/library/datetime.html) and [example](https://stackoverflow.com/questions/39846918/convert-date-to-ordinal-python) for `toordinal`. The basic process is to first convert the string to a `datetime` object using `datetime.datetime.strptime`, then convert the `datetime` object to numerical using `toordinal`.\n",
    "\n",
    "#### In the cell below, convert the `Date` column values from string to numeric values using `toordinal()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print `austin_fixed` to check your `Date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_fixed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Sampling and Holdout Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have processed the data for machine learning, we will separate the data to test and training sets.\n",
    "\n",
    "We first train the model using only the training set. We check our metrics on the training set. We then apply the model to the test set and check our metrics on the test set as well. If the metrics are significantly more optimal on the training set, then we know we have overfit our model. We will need to revise our model to ensure it will be more applicable to data outside the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cells we will separate the data into a training set and a test set using the `train_test_split()` function in scikit-learn.\n",
    "\n",
    "When using `scikit-learn` for machine learning, we first separate the data to predictor and response variables. This is the standard way of passing datasets into a model in `scikit-learn`. The `scikit-learn` will then find out whether the predictors and responses fit the model.\n",
    "\n",
    "In the next cell, assign the `TempAvgF` column to `y` and the remaining columns to `X`. Your `X` should be a subset of `austin_fixed` containing the following columns: \n",
    "\n",
    "```['Date',\n",
    " 'TempHighF',\n",
    " 'TempLowF',\n",
    " 'DewPointHighF',\n",
    " 'DewPointAvgF',\n",
    " 'DewPointLowF',\n",
    " 'HumidityHighPercent',\n",
    " 'HumidityAvgPercent',\n",
    " 'HumidityLowPercent',\n",
    " 'SeaLevelPressureHighInches',\n",
    " 'SeaLevelPressureAvgInches',\n",
    " 'SeaLevelPressureLowInches',\n",
    " 'VisibilityHighMiles',\n",
    " 'VisibilityAvgMiles',\n",
    " 'VisibilityLowMiles',\n",
    " 'WindHighMPH',\n",
    " 'WindAvgMPH',\n",
    " 'WindGustMPH',\n",
    " 'Snow',\n",
    " 'Fog',\n",
    " 'Rain',\n",
    " 'Thunderstorm']```\n",
    " \n",
    " Your `y` should be a subset of `austin_fixed` containing one column `TempAvgF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, import `train_test_split` from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data to predictor and response variables and imported the `train_test_split()` function, split `X` and `y` into `X_train`, `X_test`, `y_train`, and `y_test`. 80% of the data should be in the training set and 20% in the test set. `train_test_split()` reference can be accessed [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "\n",
    "Enter your code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations! Now you have finished the preparation of the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 1\n",
    "\n",
    "#### While the above is the common practice to prepare most datasets, when it comes to time series data, we sometimes do not want to randomly select rows from our dataset.\n",
    "\n",
    "This is because many time series algorithms rely on observations having equal time distances between them. In such cases, we typically select the majority of rows as the test data and the last few rows as the training data. We don't use `train_test_split()` to select the train/test data because it returns random selections.\n",
    "\n",
    "In the following cell, compute the number of rows that account for 80% of our data and round it to the next integer. Assign this number to `ts_rows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the first `ts_rows` rows of `X` to `X_ts_train` and the remaining rows to `X_ts_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the first `ts_rows` rows of `y` to `y_ts_train` and the remaining rows to `y_ts_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
